{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3a3ce2-caae-4f3e-83fa-4f630c26fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from dataset_3d import *\n",
    "from model_3d import *\n",
    "from resnet_2d3d import neq_load_customized\n",
    "from augmentation import *\n",
    "from utils import AverageMeter, save_checkpoint, denorm, calc_topk_accuracy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c16d18b-7419-4d0c-a36f-f745bd412879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--img_dim'], dest='img_dim', nargs=None, const=None, default=128, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--net', default='resnet18', type=str)\n",
    "parser.add_argument('--model', default='dpc-rnn', type=str)\n",
    "parser.add_argument('--dataset', default='ucf101', type=str)\n",
    "parser.add_argument('--seq_len', default=5, type=int, help='number of frames in each video block')\n",
    "parser.add_argument('--num_seq', default=8, type=int, help='number of video blocks')\n",
    "parser.add_argument('--pred_step', default=2, type=int)\n",
    "parser.add_argument('--ds', default=4, type=int, help='frame downsampling rate')\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('--wd', default=1e-5, type=float, help='weight decay')\n",
    "parser.add_argument('--resume', default='', type=str, help='path of model to resume')\n",
    "parser.add_argument('--pretrain', default='', type=str, help='path of pretrained model')\n",
    "parser.add_argument('--epochs', default=10, type=int, help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--gpu', default='0', type=str)\n",
    "parser.add_argument('--print_freq', default=5, type=int, help='frequency of printing output during training')\n",
    "parser.add_argument('--reset_lr', action='store_true', help='Reset learning rate when resume training?')\n",
    "parser.add_argument('--prefix', default='tmp', type=str, help='prefix of checkpoint filename')\n",
    "parser.add_argument('--train_what', default='all', type=str)\n",
    "parser.add_argument('--img_dim', default=128, type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75ca3d5b-ef40-46a0-bf98-97d08ecd0989",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "global args; args = parser.parse_known_args()[0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.gpu)\n",
    "global cuda; cuda = torch.device('cuda')\n",
    "\n",
    "best_acc = 0\n",
    "global iteration; iteration = 0\n",
    "args.old_lr = None\n",
    "\n",
    "def get_data(transform, mode='train'):\n",
    "    print('Loading data for \"%s\" ...' % mode)\n",
    "    if args.dataset == 'k400':\n",
    "        use_big_K400 = args.img_dim > 140\n",
    "        dataset = Kinetics400_full_3d(mode=mode,\n",
    "                              transform=transform,\n",
    "                              seq_len=args.seq_len,\n",
    "                              num_seq=args.num_seq,\n",
    "                              downsample=5,\n",
    "                              big=use_big_K400)\n",
    "    elif args.dataset == 'ucf101':\n",
    "        dataset = UCF101_3d(mode=mode,\n",
    "                         transform=transform,\n",
    "                         seq_len=args.seq_len,\n",
    "                         num_seq=args.num_seq,\n",
    "                         downsample=args.ds)\n",
    "    else:\n",
    "        raise ValueError('dataset not supported')\n",
    "\n",
    "    sampler = data.RandomSampler(dataset)\n",
    "\n",
    "    if mode == 'train':\n",
    "        data_loader = data.DataLoader(dataset,\n",
    "                                      batch_size=args.batch_size,\n",
    "                                      sampler=sampler,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=4,\n",
    "                                      pin_memory=True,\n",
    "                                      drop_last=True)\n",
    "    elif mode == 'val':\n",
    "        data_loader = data.DataLoader(dataset,\n",
    "                                      batch_size=args.batch_size,\n",
    "                                      sampler=sampler,\n",
    "                                      shuffle=False,\n",
    "                                      num_workers=4,\n",
    "                                      pin_memory=True,\n",
    "                                      drop_last=True)\n",
    "    print('\"%s\" dataset size: %d' % (mode, len(dataset)))\n",
    "    return data_loader, dataset\n",
    "\n",
    "def set_path(args):\n",
    "    if args.resume: exp_path = os.path.dirname(os.path.dirname(args.resume))\n",
    "    else:\n",
    "        exp_path = 'log_{args.prefix}/{args.dataset}-{args.img_dim}_{0}_{args.model}_\\\n",
    "bs{args.batch_size}_lr{1}_seq{args.num_seq}_pred{args.pred_step}_len{args.seq_len}_ds{args.ds}_\\\n",
    "train-{args.train_what}{2}'.format(\n",
    "                    'r%s' % args.net[6::], \\\n",
    "                    args.old_lr if args.old_lr is not None else args.lr, \\\n",
    "                    '_pt=%s' % args.pretrain.replace('/','-') if args.pretrain else '', \\\n",
    "                    args=args)\n",
    "    img_path = os.path.join(exp_path, 'img')\n",
    "    model_path = os.path.join(exp_path, 'model')\n",
    "    if not os.path.exists(img_path): os.makedirs(img_path)\n",
    "    if not os.path.exists(model_path): os.makedirs(model_path)\n",
    "    return img_path, model_path\n",
    "\n",
    "def process_output(mask):\n",
    "    '''task mask as input, compute the target for contrastive loss'''\n",
    "    # dot product is computed in parallel gpus, so get less easy neg, bounded by batch size in each gpu'''\n",
    "    # mask meaning: -2: omit, -1: temporal neg (hard), 0: easy neg, 1: pos, -3: spatial neg\n",
    "    (B, NP, SQ, B2, NS, _) = mask.size() # [B, P, SQ, B, N, SQ]\n",
    "    target = mask == 1\n",
    "    target.requires_grad = False\n",
    "    return target, (B, B2, NS, NP, SQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cadd8497-ca74-40d0-b29c-204e3b081da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for \"train\" ...\n",
      "\"train\" dataset size: 5078\n"
     ]
    }
   ],
   "source": [
    "# transformation for input (for contrastive learning)\n",
    "# from main import *\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            RandomHorizontalFlip(consistent=True),\n",
    "            RandomCrop(size=224, consistent=True),\n",
    "            Scale(size=(args.img_dim,args.img_dim)),\n",
    "            RandomGray(consistent=False, p=0.5),\n",
    "            ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.25, p=1.0),\n",
    "            ToTensor(),\n",
    "            Normalize()\n",
    "        ])\n",
    "\n",
    "train_loader, dataset = get_data(transform, 'train')\n",
    "\n",
    "global de_normalize; de_normalize = denorm()\n",
    "global img_path; img_path, model_path = set_path(args)\n",
    "global writer_train\n",
    "try: # old version\n",
    "    writer_val = SummaryWriter(log_dir=os.path.join(img_path, 'val'))\n",
    "    writer_train = SummaryWriter(log_dir=os.path.join(img_path, 'train'))\n",
    "except: # v1.7\n",
    "    writer_val = SummaryWriter(logdir=os.path.join(img_path, 'val'))\n",
    "    writer_train = SummaryWriter(logdir=os.path.join(img_path, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "623feeb6-764a-4f98-b34c-42de0849b622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 5, 128, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21cecfd6-914b-45d3-9d39-42657719a0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([64, 8, 3, 5, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input = None\n",
    "\n",
    "for idx, input_seq in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    print(idx)\n",
    "    print(input_seq.shape)\n",
    "    input = input_seq\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3de1ea81-70d1-46a5-b8bc-dc4842f94599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "sys.path.append('../backbone')\n",
    "from select_backbone import select_resnet\n",
    "from convrnn import ConvGRU\n",
    "\n",
    "\n",
    "class DPC_RNN(nn.Module):\n",
    "    '''DPC with RNN'''\n",
    "    def __init__(self, sample_size, num_seq=8, seq_len=5, pred_step=3, network='resnet50'):\n",
    "        super(DPC_RNN, self).__init__()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda:0')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        \n",
    "        torch.cuda.manual_seed(233)\n",
    "        print('Using DPC-RNN model')\n",
    "        self.sample_size = sample_size\n",
    "        self.num_seq = num_seq\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_step = pred_step\n",
    "        self.last_duration = int(math.ceil(seq_len / 4))\n",
    "        self.last_size = int(math.ceil(sample_size / 32))\n",
    "        print('final feature map has size %dx%d' % (self.last_size, self.last_size))\n",
    "\n",
    "        self.backbone, self.param = select_resnet(network, track_running_stats=False)\n",
    "        self.param['num_layers'] = 1 # param for GRU\n",
    "        self.param['hidden_size'] = self.param['feature_size'] # param for GRU\n",
    "\n",
    "        self.agg = ConvGRU(input_size=self.param['feature_size'],\n",
    "                               hidden_size=self.param['hidden_size'],\n",
    "                               kernel_size=1,\n",
    "                               num_layers=self.param['num_layers'])\n",
    "        self.network_pred = nn.Sequential(\n",
    "                                nn.Conv2d(self.param['feature_size'], self.param['feature_size'], kernel_size=1, padding=0),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Conv2d(self.param['feature_size'], self.param['feature_size'], kernel_size=1, padding=0)\n",
    "                                )\n",
    "        self.mask = None\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self._initialize_weights(self.agg)\n",
    "        self._initialize_weights(self.network_pred)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, block):\n",
    "        # block: [B, N, C, SL, W, H]\n",
    "        ### extract feature ###\n",
    "        '''\n",
    "        B : batch size\n",
    "        N : number of sequences\n",
    "        C : channels of each images\n",
    "        SL : length of sequence\n",
    "        W, H : size of images\n",
    "        '''\n",
    "        (B, N, C, SL, H, W) = block.shape\n",
    "        print(f'shape of the raw input block : {block.shape}')\n",
    "        block = block.view(B*N, C, SL, H, W)\n",
    "        print(f'shape of block after reshaping : {block.shape}')\n",
    "        feature = self.backbone(block)\n",
    "        print(f'shape of the latent vector after ResNet processing : {feature.shape}')\n",
    "        del block\n",
    "        feature = F.avg_pool3d(feature, (self.last_duration, 1, 1), stride=(1, 1, 1))\n",
    "\n",
    "        feature_inf_all = feature.view(B, N, self.param['feature_size'], self.last_size, self.last_size) # before ReLU, (-inf, +inf)\n",
    "        feature = self.relu(feature) # [0, +inf)\n",
    "        feature = feature.view(B, N, self.param['feature_size'], self.last_size, self.last_size) # [B,N,D,6,6], [0, +inf)\n",
    "        feature_inf = feature_inf_all[:, N-self.pred_step::, :].contiguous()\n",
    "        \n",
    "        del feature_inf_all\n",
    "\n",
    "        ### aggregate, predict future ###\n",
    "        # aggregate previous information\n",
    "        _, hidden = self.agg(feature[:, 0:N-self.pred_step, :].contiguous())\n",
    "        hidden = hidden[:,-1,:] # after tanh, (-1,1). get the hidden state of last layer, last time step\n",
    "        \n",
    "        \n",
    "        # predict the future\n",
    "        pred = []\n",
    "        for i in range(self.pred_step):\n",
    "            # sequentially pred future\n",
    "            p_tmp = self.network_pred(hidden)\n",
    "            pred.append(p_tmp)\n",
    "            _, hidden = self.agg(self.relu(p_tmp).unsqueeze(1), hidden.unsqueeze(0))\n",
    "            hidden = hidden[:,-1,:]\n",
    "        pred = torch.stack(pred, 1) # B, pred_step, xxx\n",
    "        del hidden\n",
    "\n",
    "\n",
    "        ### Get similarity score ###\n",
    "        # pred: [B, pred_step, D, last_size, last_size]\n",
    "        # GT: [B, N, D, last_size, last_size]\n",
    "        N = self.pred_step\n",
    "        # dot product D dimension in pred-GT pair, get a 6d tensor. First 3 dims are from pred, last 3 dims are from GT. \n",
    "        pred = pred.permute(0,1,3,4,2).contiguous().view(B*self.pred_step*self.last_size**2, self.param['feature_size'])\n",
    "        feature_inf = feature_inf.permute(0,1,3,4,2).contiguous().view(B*N*self.last_size**2, self.param['feature_size']).transpose(0,1)\n",
    "        score = torch.matmul(pred, feature_inf).view(B, self.pred_step, self.last_size**2, B, N, self.last_size**2)\n",
    "        del feature_inf, pred\n",
    "\n",
    "        if self.mask is None: # only compute mask once\n",
    "            # mask meaning: -2: omit, -1: temporal neg (hard), 0: easy neg, 1: pos, -3: spatial neg\n",
    "            mask = torch.zeros((B, self.pred_step, self.last_size**2, B, N, self.last_size**2), dtype=torch.int8, requires_grad=False).detach().cuda()\n",
    "            mask[torch.arange(B), :, :, torch.arange(B), :, :] = -3 # spatial neg\n",
    "            for k in range(B):\n",
    "                mask[k, :, torch.arange(self.last_size**2), k, :, torch.arange(self.last_size**2)] = -1 # temporal neg\n",
    "            tmp = mask.permute(0, 2, 1, 3, 5, 4).contiguous().view(B*self.last_size**2, self.pred_step, B*self.last_size**2, N)\n",
    "            for j in range(B*self.last_size**2):\n",
    "                tmp[j, torch.arange(self.pred_step), j, torch.arange(N-self.pred_step, N)] = 1 # pos\n",
    "            mask = tmp.view(B, self.last_size**2, self.pred_step, B, self.last_size**2, N).permute(0,2,1,3,5,4)\n",
    "            self.mask = mask\n",
    "\n",
    "        return [score, self.mask]\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param, 1)\n",
    "        # other resnet weights have been initialized in resnet itself\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6219cf20-d844-4d58-8ba9-78a42cd12045",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DPC-RNN model\n",
      "final feature map has size 4x4\n",
      "\n",
      "===========Check Grad============\n",
      "module.backbone.conv1.weight True\n",
      "module.backbone.bn1.weight True\n",
      "module.backbone.bn1.bias True\n",
      "module.backbone.layer1.0.conv1.weight True\n",
      "module.backbone.layer1.0.bn1.weight True\n",
      "module.backbone.layer1.0.bn1.bias True\n",
      "module.backbone.layer1.0.conv2.weight True\n",
      "module.backbone.layer1.0.bn2.weight True\n",
      "module.backbone.layer1.0.bn2.bias True\n",
      "module.backbone.layer1.1.conv1.weight True\n",
      "module.backbone.layer1.1.bn1.weight True\n",
      "module.backbone.layer1.1.bn1.bias True\n",
      "module.backbone.layer1.1.conv2.weight True\n",
      "module.backbone.layer1.1.bn2.weight True\n",
      "module.backbone.layer1.1.bn2.bias True\n",
      "module.backbone.layer2.0.conv1.weight True\n",
      "module.backbone.layer2.0.bn1.weight True\n",
      "module.backbone.layer2.0.bn1.bias True\n",
      "module.backbone.layer2.0.conv2.weight True\n",
      "module.backbone.layer2.0.bn2.weight True\n",
      "module.backbone.layer2.0.bn2.bias True\n",
      "module.backbone.layer2.0.downsample.0.weight True\n",
      "module.backbone.layer2.0.downsample.1.weight True\n",
      "module.backbone.layer2.0.downsample.1.bias True\n",
      "module.backbone.layer2.1.conv1.weight True\n",
      "module.backbone.layer2.1.bn1.weight True\n",
      "module.backbone.layer2.1.bn1.bias True\n",
      "module.backbone.layer2.1.conv2.weight True\n",
      "module.backbone.layer2.1.bn2.weight True\n",
      "module.backbone.layer2.1.bn2.bias True\n",
      "module.backbone.layer3.0.conv1.weight True\n",
      "module.backbone.layer3.0.bn1.weight True\n",
      "module.backbone.layer3.0.bn1.bias True\n",
      "module.backbone.layer3.0.conv2.weight True\n",
      "module.backbone.layer3.0.bn2.weight True\n",
      "module.backbone.layer3.0.bn2.bias True\n",
      "module.backbone.layer3.0.downsample.0.weight True\n",
      "module.backbone.layer3.0.downsample.1.weight True\n",
      "module.backbone.layer3.0.downsample.1.bias True\n",
      "module.backbone.layer3.1.conv1.weight True\n",
      "module.backbone.layer3.1.bn1.weight True\n",
      "module.backbone.layer3.1.bn1.bias True\n",
      "module.backbone.layer3.1.conv2.weight True\n",
      "module.backbone.layer3.1.bn2.weight True\n",
      "module.backbone.layer3.1.bn2.bias True\n",
      "module.backbone.layer4.0.conv1.weight True\n",
      "module.backbone.layer4.0.bn1.weight True\n",
      "module.backbone.layer4.0.bn1.bias True\n",
      "module.backbone.layer4.0.conv2.weight True\n",
      "module.backbone.layer4.0.bn2.weight True\n",
      "module.backbone.layer4.0.bn2.bias True\n",
      "module.backbone.layer4.0.downsample.0.weight True\n",
      "module.backbone.layer4.0.downsample.1.weight True\n",
      "module.backbone.layer4.0.downsample.1.bias True\n",
      "module.backbone.layer4.1.conv1.weight True\n",
      "module.backbone.layer4.1.bn1.weight True\n",
      "module.backbone.layer4.1.bn1.bias True\n",
      "module.backbone.layer4.1.conv2.weight True\n",
      "module.backbone.layer4.1.bn2.weight True\n",
      "module.backbone.layer4.1.bn2.bias True\n",
      "module.agg.ConvGRUCell_00.reset_gate.weight True\n",
      "module.agg.ConvGRUCell_00.reset_gate.bias True\n",
      "module.agg.ConvGRUCell_00.update_gate.weight True\n",
      "module.agg.ConvGRUCell_00.update_gate.bias True\n",
      "module.agg.ConvGRUCell_00.out_gate.weight True\n",
      "module.agg.ConvGRUCell_00.out_gate.bias True\n",
      "module.network_pred.0.weight True\n",
      "module.network_pred.0.bias True\n",
      "module.network_pred.2.weight True\n",
      "module.network_pred.2.bias True\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### dpc model ###\n",
    "if args.model == 'dpc-rnn':\n",
    "    model = DPC_RNN(sample_size=args.img_dim, \n",
    "                    num_seq=args.num_seq, \n",
    "                    seq_len=args.seq_len, \n",
    "                    network=args.net, \n",
    "                    pred_step=args.pred_step)\n",
    "else: raise ValueError('wrong model!')\n",
    "    \n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(cuda)\n",
    "global criterion; criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### optimizer ###\n",
    "if args.train_what == 'last':\n",
    "    for name, param in model.module.resnet.named_parameters():\n",
    "        param.requires_grad = False\n",
    "else: pass # train all layers\n",
    "\n",
    "print('\\n===========Check Grad============')\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "print('=================================\\n')\n",
    "\n",
    "params = model.parameters()\n",
    "optimizer = optim.Adam(params, lr=args.lr, weight_decay=args.wd)\n",
    "args.old_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "47f8c70f-6e0f-4de6-b91c-2eddbe9c2157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yiranwang/Workplace/DPC/UCF101/frame/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03/\n"
     ]
    }
   ],
   "source": [
    "a, b = train_loader.dataset.video_info.iloc[0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02f694dd-4e03-430a-bde4-83e3e3198c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the raw input block : torch.Size([4, 8, 3, 5, 128, 128])\n",
      "shape of block after reshaping : torch.Size([32, 3, 5, 128, 128])\n",
      "shape of the latent vector after ResNet processing : torch.Size([32, 256, 2, 4, 4])\n",
      "torch.Size([4, 3, 16, 4, 3, 16])\n",
      "torch.Size([4, 3, 16, 4, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "score_, mask_ = model(input)\n",
    "print(score.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "target_, (_, B2, NS, NP, SQ) = process_output(mask_)\n",
    "\n",
    "B = input.size(0)\n",
    "\n",
    "score_flattened = score_.view(B*NP*SQ, B2*NS*SQ)\n",
    "# print(f'target shape : {target_.shape}')\n",
    "# print(B, NP, SQ, B2, NS)\n",
    "target_flattened = target_.contiguous().view(B*NP*SQ, B2*NS*SQ)\n",
    "# print(target_flattened)\n",
    "target_flattened = target_flattened.double().argmax(dim=1)\n",
    "\n",
    "loss = criterion(score_flattened, target_flattened)\n",
    "\n",
    "top1, top3, top5 = calc_topk_accuracy(score_flattened, target_flattened, (1,3,5))\n",
    "\n",
    "# the latent representation is of shape [batch-size, out-channels, D(third dimension), H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d91eb002-1ee6-4be4-9f51-c52a3a18021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 192])\n",
      "torch.Size([192])\n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191], device='cuda:0')\n",
      "torch.Size([4, 3, 16, 4, 3, 16])\n",
      "torch.Size([4, 3, 16, 4, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "print(score_flattened.shape)\n",
    "print(target_flattened.shape)\n",
    "print(target_flattened)\n",
    "print((mask_==1).shape)\n",
    "print(target_.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPC",
   "language": "python",
   "name": "dpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
