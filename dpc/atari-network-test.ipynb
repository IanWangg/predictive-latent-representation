{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8261e8df-81b2-403d-ab9e-1b7ce4cf7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "sys.path.append('../backbone')\n",
    "from select_backbone import select_resnet\n",
    "from convrnn import ConvGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e624d58e-2055-430c-b3f4-76c87f5272d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DPC_RNN(nn.Module):\n",
    "    '''DPC with RNN'''\n",
    "    def __init__(self, sample_size, num_seq=8, seq_len=5, pred_step=3, network='resnet50'):\n",
    "        super(DPC_RNN, self).__init__()\n",
    "        torch.cuda.manual_seed(233)\n",
    "        print('Using DPC-RNN model')\n",
    "        self.sample_size = sample_size\n",
    "        self.num_seq = num_seq\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_step = pred_step\n",
    "        self.last_duration = int(math.ceil(seq_len / 4))\n",
    "        self.last_size = int(math.ceil(sample_size / 32))\n",
    "        print('final feature map has size %dx%d' % (self.last_size, self.last_size))\n",
    "\n",
    "        self.backbone, self.param = select_resnet(network, track_running_stats=False)\n",
    "        self.param['num_layers'] = 1 # param for GRU\n",
    "        self.param['hidden_size'] = self.param['feature_size'] # param for GRU\n",
    "\n",
    "        self.agg = ConvGRU(input_size=self.param['feature_size'],\n",
    "                               hidden_size=self.param['hidden_size'],\n",
    "                               kernel_size=1,\n",
    "                               num_layers=self.param['num_layers'])\n",
    "        self.network_pred = nn.Sequential(\n",
    "                                nn.Conv2d(self.param['feature_size'], self.param['feature_size'], kernel_size=1, padding=0),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Conv2d(self.param['feature_size'], self.param['feature_size'], kernel_size=1, padding=0)\n",
    "                                )\n",
    "        self.mask = None\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self._initialize_weights(self.agg)\n",
    "        self._initialize_weights(self.network_pred)\n",
    "\n",
    "    def forward(self, block):\n",
    "        # block: [B, N, C, SL, W, H]\n",
    "        ### extract feature ###\n",
    "        '''\n",
    "        B : batch size\n",
    "        N : number of sequences\n",
    "        C : channels of each images\n",
    "        SL : length of sequence\n",
    "        W, H : size of images\n",
    "        '''\n",
    "        (B, N, C, SL, H, W) = block.shape\n",
    "        block = block.view(B*N, C, SL, H, W)\n",
    "        feature = self.backbone(block)\n",
    "#         print(feature.shape)\n",
    "        del block\n",
    "        feature = F.avg_pool3d(feature, (self.last_duration, 1, 1), stride=(1, 1, 1))\n",
    "\n",
    "        feature_inf_all = feature.view(B, N, self.param['feature_size'], self.last_size, self.last_size) # before ReLU, (-inf, +inf)\n",
    "        feature = self.relu(feature) # [0, +inf)\n",
    "        feature = feature.view(B, N, self.param['feature_size'], self.last_size, self.last_size) # [B,N,D,6,6], [0, +inf)\n",
    "        feature_inf = feature_inf_all[:, N-self.pred_step::, :].contiguous()\n",
    "        del feature_inf_all\n",
    "\n",
    "        ### aggregate, predict future ###\n",
    "        # aggregate previous information\n",
    "        _, hidden = self.agg(feature[:, 0:N-self.pred_step, :].contiguous())\n",
    "        hidden = hidden[:,-1,:] # after tanh, (-1,1). get the hidden state of last layer, last time step\n",
    "        \n",
    "        # predict the future\n",
    "        pred = []\n",
    "        for i in range(self.pred_step):\n",
    "            # sequentially pred future\n",
    "            p_tmp = self.network_pred(hidden)\n",
    "            pred.append(p_tmp)\n",
    "            _, hidden = self.agg(self.relu(p_tmp).unsqueeze(1), hidden.unsqueeze(0))\n",
    "            hidden = hidden[:,-1,:]\n",
    "        pred = torch.stack(pred, 1) # B, pred_step, xxx\n",
    "        del hidden\n",
    "\n",
    "\n",
    "        ### Get similarity score ###\n",
    "        # pred: [B, pred_step, D, last_size, last_size]\n",
    "        # GT: [B, N, D, last_size, last_size]\n",
    "        N = self.pred_step\n",
    "        # dot product D dimension in pred-GT pair, get a 6d tensor. First 3 dims are from pred, last 3 dims are from GT. \n",
    "        pred = pred.permute(0,1,3,4,2).contiguous().view(B*self.pred_step*self.last_size**2, self.param['feature_size'])\n",
    "        feature_inf = feature_inf.permute(0,1,3,4,2).contiguous().view(B*N*self.last_size**2, self.param['feature_size']).transpose(0,1)\n",
    "        score = torch.matmul(pred, feature_inf).view(B, self.pred_step, self.last_size**2, B, N, self.last_size**2)\n",
    "        del feature_inf, pred\n",
    "\n",
    "        if self.mask is None: # only compute mask once\n",
    "            # mask meaning: -2: omit, -1: temporal neg (hard), 0: easy neg, 1: pos, -3: spatial neg\n",
    "            mask = torch.zeros((B, self.pred_step, self.last_size**2, B, N, self.last_size**2), dtype=torch.int8, requires_grad=False).detach().cuda()\n",
    "            mask[torch.arange(B), :, :, torch.arange(B), :, :] = -3 # spatial neg\n",
    "            for k in range(B):\n",
    "                mask[k, :, torch.arange(self.last_size**2), k, :, torch.arange(self.last_size**2)] = -1 # temporal neg\n",
    "            tmp = mask.permute(0, 2, 1, 3, 5, 4).contiguous().view(B*self.last_size**2, self.pred_step, B*self.last_size**2, N)\n",
    "            for j in range(B*self.last_size**2):\n",
    "                tmp[j, torch.arange(self.pred_step), j, torch.arange(N-self.pred_step, N)] = 1 # pos\n",
    "            mask = tmp.view(B, self.last_size**2, self.pred_step, B, self.last_size**2, N).permute(0,2,1,3,5,4)\n",
    "            self.mask = mask\n",
    "\n",
    "        return [score, self.mask]\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param, 1)\n",
    "        # other resnet weights have been initialized in resnet itself\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01e285a-7a8e-40d6-be60-a65a9865e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /home/yiranwang/.d4rl/datasets/Boxing/1/50/observation.gz...\n",
      "loading /home/yiranwang/.d4rl/datasets/Boxing/1/50/action.gz...\n",
      "loading /home/yiranwang/.d4rl/datasets/Boxing/1/50/reward.gz...\n",
      "loading /home/yiranwang/.d4rl/datasets/Boxing/1/50/terminal.gz...\n",
      "total trajactories : 969\n"
     ]
    }
   ],
   "source": [
    "from dataset_atari import Atari\n",
    "import gym\n",
    "import sys\n",
    "sys.path.append('../d4rl-atari')\n",
    "import d4rl_atari\n",
    "\n",
    "try:\n",
    "    del env\n",
    "except:\n",
    "    pass\n",
    "env = gym.make('boxing-expert-v0', stack=True)\n",
    "dataset = env.get_dataset(n_channels=4)\n",
    "\n",
    "atari = Atari(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d01cd3-e9e4-4396-a032-0c5bb604e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7]\n",
      "[3 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4, 5, 6, 7],dtype=np.int)\n",
    "i = np.array([2, 4, 5])\n",
    "print(a)\n",
    "print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9f9d09-e5d2-4eae-ba43-ed2fcabb35d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trajactories : 969\n"
     ]
    }
   ],
   "source": [
    "atari = Atari(dataset=dataset, return_actions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee3a400-4bb9-4d2e-b505-6c24990cdc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "actions = dataset['actions']\n",
    "actions.shape\n",
    "\n",
    "print(type(actions))\n",
    "\n",
    "observations = dataset['observations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd480d4b-b3c2-46b8-a597-a8ef54c0f5d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7b9076c11abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seq_start = 0\n",
    "\n",
    "index = np.array(range(seq_start, seq_start + 6, 4), dtype=np.int)\n",
    "\n",
    "\n",
    "observations[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f26be26-d1a7-40d6-a800-99d9280af301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(observations))\n",
    "print(type(observations[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22af863f-9245-4396-a489-764c79a73989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atari.__getitem__(index=0)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d7cbb4-6941-4a32-a155-c70bf92e253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[ 1  4  7 10 13 16 19]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array(range(20))\n",
    "print(a)\n",
    "\n",
    "print(a[1:20:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf44975-5050-4c9e-aaef-6c1163e77187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from dataset_3d import *\n",
    "from model_3d import *\n",
    "from resnet_2d3d import neq_load_customized\n",
    "from augmentation import *\n",
    "from utils import AverageMeter, save_checkpoint, denorm, calc_topk_accuracy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52e0793-d9e1-45f8-9478-8bccce454071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--img_dim'], dest='img_dim', nargs=None, const=None, default=128, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--net', default='resnet18', type=str)\n",
    "parser.add_argument('--model', default='dpc-rnn', type=str)\n",
    "parser.add_argument('--dataset', default='ucf101', type=str)\n",
    "parser.add_argument('--seq_len', default=5, type=int, help='number of frames in each video block')\n",
    "parser.add_argument('--num_seq', default=8, type=int, help='number of video blocks')\n",
    "parser.add_argument('--pred_step', default=3, type=int)\n",
    "parser.add_argument('--ds', default=3, type=int, help='frame downsampling rate')\n",
    "parser.add_argument('--batch_size', default=4, type=int)\n",
    "parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('--wd', default=1e-5, type=float, help='weight decay')\n",
    "parser.add_argument('--resume', default='', type=str, help='path of model to resume')\n",
    "parser.add_argument('--pretrain', default='', type=str, help='path of pretrained model')\n",
    "parser.add_argument('--epochs', default=10, type=int, help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--gpu', default='0,1', type=str)\n",
    "parser.add_argument('--print_freq', default=5, type=int, help='frequency of printing output during training')\n",
    "parser.add_argument('--reset_lr', action='store_true', help='Reset learning rate when resume training?')\n",
    "parser.add_argument('--prefix', default='tmp', type=str, help='prefix of checkpoint filename')\n",
    "parser.add_argument('--train_what', default='all', type=str)\n",
    "parser.add_argument('--img_dim', default=128, type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c270319-b56b-4c8c-8778-e1ffe6ddfaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DPC-RNN model\n",
      "final feature map has size 4x4\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "global args; args = parser.parse_known_args()[0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.gpu)\n",
    "global cuda; cuda = torch.device('cuda')\n",
    "\n",
    "best_acc = 0\n",
    "global iteration; iteration = 0\n",
    "\n",
    "if args.model == 'dpc-rnn':\n",
    "    model = DPC_RNN(sample_size=args.img_dim, \n",
    "                    num_seq=args.num_seq, \n",
    "                    seq_len=args.seq_len, \n",
    "                    network=args.net, \n",
    "                    pred_step=args.pred_step)\n",
    "else: raise ValueError('wrong model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f16399c-0cae-4e80-ac58-c481bf64a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "sampler = data.RandomSampler(dataset)\n",
    "\n",
    "data_loader = data.DataLoader(atari,\n",
    "                             batch_size=4,\n",
    "                             sampler=sampler,\n",
    "                             shuffle=False,\n",
    "                             num_workers=4,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d396bc75-b554-4176-9f7b-feeb818e4bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 5, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "for idx, X in enumerate(data_loader):\n",
    "    data = X\n",
    "    index = idx\n",
    "    break\n",
    "    \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cb543b-10ba-4396-8518-f1569c1154a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 84, 84])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atari[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPC",
   "language": "python",
   "name": "dpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
