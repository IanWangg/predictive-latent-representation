{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780e91a1-e8ea-4d2a-af74-3c7531b70a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yiranwang/anaconda3/envs/DPC/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from deep_rl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b46e05c-ce24-4a28-9092-e23c5a930a03",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'observations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-09caa550cc82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreplay_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'observations'"
     ]
    }
   ],
   "source": [
    "replay_buffer = {}\n",
    "replay_buffer['observations']\n",
    "replay_buffer['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd2a14-01c7-4b38-a0d2-cd4dde431a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNActor(BaseActor):\n",
    "    def __init__(self, config):\n",
    "        BaseActor.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.start()\n",
    "\n",
    "    def _transition(self):\n",
    "        if self._state is None:\n",
    "            self._state = self._task.reset()\n",
    "        config = self.config\n",
    "        with config.lock:\n",
    "            q_values = self._network(config.state_normalizer(self._state))\n",
    "        q_values = to_np(q_values).flatten()\n",
    "        if self._total_steps < config.exploration_steps \\\n",
    "                or np.random.rand() < config.random_action_prob():\n",
    "            action = np.random.randint(0, len(q_values))\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        next_state, reward, done, info = self._task.step([action])\n",
    "        entry = [self._state[0], action, reward[0], next_state[0], int(done[0]), info]\n",
    "        self._total_steps += 1\n",
    "        self._state = next_state\n",
    "        return entry\n",
    "\n",
    "\n",
    "class DQNAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        config.lock = mp.Lock()\n",
    "\n",
    "        self.returns = []\n",
    "        \n",
    "        self.replay = config.replay_fn()\n",
    "        self.actor = DQNActor(config)\n",
    "\n",
    "        self.network = config.network_fn()\n",
    "        self.network.share_memory()\n",
    "        self.target_network = config.network_fn()\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = config.optimizer_fn(self.network.parameters())\n",
    "        self.loss_vec = []\n",
    "\n",
    "        self.actor.set_network(self.network)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.batch_indices = range_tensor(self.replay.batch_size)\n",
    "        \n",
    "        try:\n",
    "            self.is_wb = config.is_wb\n",
    "        except:\n",
    "            self.is_wb = False\n",
    "            \n",
    "        if(self.is_wb):\n",
    "            wandb.init(entity=\"psurya\", project=\"sample-project\")\n",
    "            wandb.watch_called = False\n",
    "\n",
    "    def close(self):\n",
    "        close_obj(self.replay)\n",
    "        close_obj(self.actor)\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        self.config.state_normalizer.set_read_only()\n",
    "        state = self.config.state_normalizer(state)\n",
    "        q = self.network(state)\n",
    "        action = to_np(q.argmax(-1))\n",
    "        self.config.state_normalizer.unset_read_only()\n",
    "        return action\n",
    "\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        transitions = self.actor.step()\n",
    "        experiences = []\n",
    "        for state, action, reward, next_state, done, info in transitions:\n",
    "            # self.record_online_return(info)\n",
    "            \n",
    "            # Recording train returns in list\n",
    "            for i, info_ in enumerate(info):\n",
    "                ret = info_['episodic_return']\n",
    "                if ret is not None:\n",
    "                    self.returns.append([self.total_steps, ret])\n",
    "                    if(self.is_wb):\n",
    "                        wandb.log({\"steps_ret\": self.total_steps, \"returns\": ret})\n",
    "            \n",
    "            self.total_steps += 1\n",
    "            reward = config.reward_normalizer(reward)\n",
    "            experiences.append([state, action, reward, next_state, done])\n",
    "        self.replay.feed_batch(experiences)\n",
    "\n",
    "        if self.total_steps > self.config.exploration_steps:\n",
    "            experiences = self.replay.sample()\n",
    "            states, actions, rewards, next_states, terminals = experiences\n",
    "            states = self.config.state_normalizer(states)\n",
    "            next_states = self.config.state_normalizer(next_states)\n",
    "            q_next = self.target_network(next_states).detach()\n",
    "            if self.config.double_q:\n",
    "                best_actions = torch.argmax(self.network(next_states), dim=-1)\n",
    "                q_next = q_next[self.batch_indices, best_actions]\n",
    "            else:\n",
    "                q_next = q_next.max(1)[0]\n",
    "            terminals = tensor(terminals)\n",
    "            rewards = tensor(rewards)\n",
    "            q_next = self.config.discount * q_next * (1 - terminals)\n",
    "            q_next.add_(rewards)\n",
    "            actions = tensor(actions).long()\n",
    "            q = self.network(states)\n",
    "            q = q[self.batch_indices, actions]\n",
    "            loss = (q_next - q).pow(2).mul(0.5).mean()\n",
    "            # self.loss_vec.append(loss.item())\n",
    "            if(self.is_wb):\n",
    "                wandb.log({\"steps_loss\": self.total_steps, \"loss\": loss.item()})\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.config.gradient_clip)\n",
    "            with config.lock:\n",
    "                self.optimizer.step()\n",
    "\n",
    "        if self.total_steps / self.config.sgd_update_frequency % \\\n",
    "                self.config.target_network_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPC",
   "language": "python",
   "name": "dpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
